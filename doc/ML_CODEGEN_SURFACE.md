# Codegen Meta Macro Processor: ML Surface

## What the Meta Macro Processor Generates

With the ML-first architecture, the **Codegen Meta Macro Processor** has a complete, well-defined surface to generate optimized ML code.

---

## Generation Surface: ML Descriptors → Optimized Code

### 1. Model Builder Generation

**Input**: Model descriptor (declarative configuration)

```rust
// User writes descriptor
#[derive(ModelBuilder)]
pub struct DecisionTreeDescriptor {
    pub max_depth: usize,
    pub min_samples_split: usize,
    pub min_samples_leaf: usize,
    pub criterion: SplitCriterion,
    pub max_features: Option<MaxFeatures>,
}
```

**Output**: Codegen produces optimized model builder

```rust
// Generated by macro processor
impl DecisionTreeClassifier {
    pub fn from_descriptor(desc: &DecisionTreeDescriptor) -> Self {
        Self {
            max_depth: desc.max_depth,
            min_samples_split: desc.min_samples_split,
            min_samples_leaf: desc.min_samples_leaf,
            criterion: desc.criterion.clone(),
            max_features: desc.max_features.clone(),
            tree: None, // Built during training
        }
    }

    // Specialized builder for known configurations
    pub fn new_depth_10_gini() -> Self {
        // Compile-time specialized for max_depth=10, Gini criterion
        Self {
            max_depth: 10,
            min_samples_split: 2,
            min_samples_leaf: 1,
            criterion: SplitCriterion::Gini,
            max_features: None,
            tree: None,
        }
    }
}
```

**Why**: Type-safe builders, specialized implementations for common configs.

---

### 2. Training Loop Generation

**Input**: Training descriptor

```rust
#[derive(TrainingLoopGenerator)]
pub struct TrainingDescriptor {
    pub model: ModelType,
    pub optimizer: OptimizerConfig,
    pub loss: LossFunction,
    pub epochs: usize,
    pub batch_size: usize,
    pub early_stopping: Option<EarlyStoppingConfig>,
}

pub enum OptimizerConfig {
    SGD { learning_rate: f64, momentum: f64 },
    Adam { learning_rate: f64, beta1: f64, beta2: f64 },
}
```

**Output**: Codegen produces specialized training loop

```rust
// Generated training loop for Adam optimizer + CrossEntropy loss
pub fn train_adam_cross_entropy(
    model: &mut dyn Classifier,
    examples: &[TrainingExample],
    config: &TrainingConfig,
) -> Result<TrainingStats, TrainingError> {
    let mut optimizer = AdamOptimizer::new(0.001, 0.9, 0.999);
    let loss_fn = CrossEntropyLoss::new();

    for epoch in 0..config.epochs {
        let mut epoch_loss = 0.0;

        // Batch iteration (unrolled for small batches)
        for batch in examples.chunks(config.batch_size) {
            // Forward pass
            let predictions = batch.iter()
                .map(|ex| model.predict(&ex.features))
                .collect::<Result<Vec<_>, _>>()?;

            // Loss computation (vectorized)
            let batch_loss = loss_fn.compute_batch(&predictions, batch);
            epoch_loss += batch_loss;

            // Backward pass + optimizer step
            let gradients = loss_fn.compute_gradients(&predictions, batch);
            optimizer.step(model, &gradients)?;
        }

        // Early stopping check
        if config.early_stopping.should_stop(epoch_loss) {
            break;
        }
    }

    Ok(TrainingStats { /* ... */ })
}
```

**Why**: Eliminates runtime branching, vectorized operations, specialized for optimizer/loss combination.

---

### 3. Feature Pipeline Generation

**Input**: Feature descriptor

```rust
#[derive(FeaturePipelineGenerator)]
pub struct FeatureDescriptor {
    pub source_properties: Vec<String>,
    pub transformations: Vec<TransformationType>,
    pub normalization: Option<NormalizationType>,
    pub missing_value_strategy: MissingValueStrategy,
}

pub enum TransformationType {
    OneHotEncode { categories: Vec<String> },
    LabelEncode { mapping: HashMap<String, i64> },
    PolynomialFeatures { degree: usize },
}
```

**Output**: Codegen produces optimized feature extraction

```rust
// Generated feature pipeline for known schema
pub fn extract_features_optimized(
    state: &PipelineState,
    node_ids: &[u64],
) -> Result<Vec<Features>, FeatureError> {
    let mut features = Vec::with_capacity(node_ids.len());

    // Pre-fetch property values (cache locality)
    let age_prop = state.features.get("age").unwrap();
    let income_prop = state.features.get("income").unwrap();
    let category_prop = state.features.get("category").unwrap();

    for &node_id in node_ids {
        let mut feature_vec = Vec::with_capacity(5); // Known size at compile time

        // Age (normalized)
        let age = age_prop.long_value(node_id)? as f64;
        feature_vec.push((age - 35.0) / 15.0); // Pre-computed normalization

        // Income (log-scaled)
        let income = income_prop.double_value(node_id)?;
        feature_vec.push((income.ln() - 10.5) / 1.2);

        // Category (one-hot encoded, compile-time known categories)
        let category = category_prop.long_value(node_id)?;
        feature_vec.push(if category == 0 { 1.0 } else { 0.0 }); // Category A
        feature_vec.push(if category == 1 { 1.0 } else { 0.0 }); // Category B
        feature_vec.push(if category == 2 { 1.0 } else { 0.0 }); // Category C

        features.push(Features::new(feature_vec));
    }

    Ok(features)
}
```

**Why**: Vectorized, cache-friendly, compile-time known schema eliminates runtime type checks.

---

### 4. Evaluation Code Generation

**Input**: Evaluation descriptor

```rust
#[derive(EvaluationGenerator)]
pub struct EvaluationDescriptor {
    pub metrics: Vec<MetricType>,
    pub cross_validation: Option<CrossValidationConfig>,
}

pub enum CrossValidationConfig {
    KFold { k: usize, stratified: bool },
    LeaveOneOut,
}
```

**Output**: Codegen produces optimized evaluation

```rust
// Generated 5-fold cross-validation with parallelism
pub fn evaluate_5fold_stratified(
    model_builder: impl Fn() -> Box<dyn Classifier>,
    examples: &[TrainingExample],
) -> Result<EvaluationResults, EvaluationError> {
    use rayon::prelude::*;

    // Stratified splitting (compile-time k=5)
    let folds = create_stratified_folds(examples, 5)?;

    // Parallel fold evaluation
    let fold_results: Vec<_> = folds.par_iter().enumerate()
        .map(|(i, (train, test))| {
            let mut model = model_builder();
            model.train(train)?;

            let predictions = test.iter()
                .map(|ex| model.predict(&ex.features))
                .collect::<Result<Vec<_>, _>>()?;

            // Compute metrics (vectorized)
            Ok(MetricsComputer::compute_all(&predictions, test))
        })
        .collect::<Result<Vec<_>, _>>()?;

    // Aggregate results
    Ok(EvaluationResults::aggregate(&fold_results))
}
```

**Why**: Parallel execution, compile-time fold count, specialized aggregation.

---

## FormProcessor Integration: What Gets Generated

### 1. Progress Tracking Code

**Input**: Progress tracking descriptor

```rust
pub struct ProgressConfig {
    pub report_interval: ReportInterval,
    pub metrics_to_track: Vec<MetricType>,
}
```

**Output**: Generated progress reporting

```rust
// Generated progress tracker
pub struct GeneratedProgressTracker {
    reporter: Arc<dyn ProgressReporter>,
    last_report: Instant,
    interval: Duration,
}

impl GeneratedProgressTracker {
    pub fn report_if_needed(&mut self, epoch: usize, loss: f64, metrics: &Metrics) {
        if self.last_report.elapsed() >= self.interval {
            self.reporter.report_epoch(epoch, loss, metrics);
            self.last_report = Instant::now();
        }
    }
}

// Integration into training loop
for epoch in 0..epochs {
    // ... training ...
    progress_tracker.report_if_needed(epoch, epoch_loss, &metrics);
}
```

### 2. Resource Management Code

**Input**: Resource limits descriptor

```rust
pub struct ResourceLimitsDescriptor {
    pub max_feature_matrix_mb: usize,
    pub max_training_minutes: usize,
    pub max_concurrent_models: usize,
}
```

**Output**: Generated resource checks

```rust
// Generated resource estimator
pub struct GeneratedResourceEstimator;

impl GeneratedResourceEstimator {
    pub fn estimate_feature_matrix_size(
        num_samples: usize,
        num_features: usize,
    ) -> usize {
        // Each feature is f64 (8 bytes)
        num_samples * num_features * 8
    }

    pub fn check_limits(
        num_samples: usize,
        num_features: usize,
        limits: &ResourceLimits,
    ) -> Result<(), ResourceError> {
        let estimated_mb = Self::estimate_feature_matrix_size(num_samples, num_features) / (1024 * 1024);

        if estimated_mb > limits.max_feature_matrix_mb {
            return Err(ResourceError::MemoryLimitExceeded {
                requested: estimated_mb,
                limit: limits.max_feature_matrix_mb,
            });
        }

        Ok(())
    }
}

// Integration into training executor
GeneratedResourceEstimator::check_limits(examples.len(), feature_dim, &limits)?;
```

---

## Codegen Architecture Pattern

### The Three-Layer Pattern

```
┌────────────────────────────────────────────────────────────┐
│                     Layer 1: Descriptors                   │
│                  (User-written, declarative)               │
│                                                            │
│  ModelDescriptor, TrainingDescriptor, FeatureDescriptor   │
└────────────────────────┬───────────────────────────────────┘
                         │
                         ▼
┌────────────────────────────────────────────────────────────┐
│              Layer 2: Meta Macro Processor                 │
│                    (Code generation)                       │
│                                                            │
│  Analyzes descriptors → Generates specialized code        │
│  - Model builders                                         │
│  - Training loops                                         │
│  - Feature pipelines                                      │
│  - Evaluation code                                        │
└────────────────────────┬───────────────────────────────────┘
                         │
                         ▼
┌────────────────────────────────────────────────────────────┐
│           Layer 3: Generated Optimized Code                │
│              (Compiled, specialized, fast)                 │
│                                                            │
│  Specialized implementations for specific configurations   │
│  - Vectorized operations                                  │
│  - Compile-time optimizations                             │
│  - Zero-cost abstractions                                 │
└────────────────────────────────────────────────────────────┘
```

---

## Example: End-to-End Codegen Flow

### User Writes Pipeline Descriptor

```rust
let pipeline = PipelineDescriptor {
    name: "customer_churn_prediction",
    steps: vec![
        // Feature extraction (could call graph procedures)
        FeatureStepDescriptor {
            name: "extract_features",
            sources: vec!["age", "income", "engagement_score"],
            transformations: vec![
                Normalize::ZScore,
                OneHotEncode { categories: vec!["A", "B", "C"] },
            ],
        },
    ],
    training: Some(TrainingDescriptor {
        model: ModelType::DecisionTreeClassifier,
        target_property: "churned",
        optimization: OptimizationConfig {
            optimizer: Optimizer::Adam { lr: 0.001 },
            loss: LossFunction::CrossEntropy,
            epochs: 100,
            batch_size: 32,
        },
        validation: ValidationConfig {
            split: SplitConfig::TrainTest { train_fraction: 0.8 },
            metrics: vec![Accuracy, F1, AUCROC],
        },
    }),
};
```

### Meta Macro Processor Generates

```rust
// Generated specialized pipeline executor
pub mod generated_customer_churn_pipeline {
    use super::*;

    pub struct ChurnPredictionPipeline {
        model: DecisionTreeClassifier,
        feature_extractor: GeneratedFeatureExtractor,
    }

    impl ChurnPredictionPipeline {
        pub fn train(&mut self, state: &PipelineState, node_ids: &[u64])
            -> Result<TrainingResult, PipelineError>
        {
            // Generated feature extraction
            let features = self.feature_extractor.extract(state, node_ids)?;

            // Generated training loop (Adam + CrossEntropy)
            let stats = train_adam_cross_entropy(&mut self.model, &features)?;

            // Generated validation
            let metrics = evaluate_train_test_split(&self.model, &features, 0.8)?;

            Ok(TrainingResult { stats, metrics })
        }
    }

    struct GeneratedFeatureExtractor;

    impl GeneratedFeatureExtractor {
        fn extract(&self, state: &PipelineState, node_ids: &[u64])
            -> Result<Vec<TrainingExample>, FeatureError>
        {
            // Specialized extraction (compile-time schema)
            // Vectorized, cache-friendly
            // ... (optimized code from earlier example) ...
        }
    }
}
```

### FormProcessor Orchestrates

```rust
// FormProcessor uses generated code
let mut pipeline = generated_customer_churn_pipeline::ChurnPredictionPipeline::new();

let result = pipeline.train(&pipeline_state, &node_ids)?;

// FormProcessor receives progress events during training
// - EpochCompleted { epoch: 0, loss: 0.693, ... }
// - EpochCompleted { epoch: 1, loss: 0.521, ... }
// - ValidationCompleted { accuracy: 0.85, f1: 0.82, ... }
// - TrainingCompleted { final_metrics: ..., duration: 23.4s }
```

---

## Key Codegen Benefits

### 1. Performance

- **Compile-time specialization**: Code generated for specific configurations
- **Vectorization**: Batch operations, SIMD where possible
- **Zero-cost abstractions**: Trait objects only where needed, static dispatch elsewhere
- **Cache-friendly**: Generated code optimizes memory layout

### 2. Correctness

- **Type safety**: Descriptors ensure valid configurations
- **Compile-time validation**: Invalid descriptors → compile errors
- **Reproducibility**: Same descriptor → same generated code → same results

### 3. Observability

- **FormProcessor integration**: Progress tracking baked into generated code
- **Resource management**: Limit checks at generation time
- **Error context**: Generated code includes descriptor info in errors

### 4. Extensibility

- **Add new models**: Implement model trait + descriptor → codegen works
- **Add new optimizers**: Implement optimizer trait + descriptor → codegen generates loop
- **Add new transforms**: Implement transform trait + descriptor → codegen generates pipeline

---

## What This Enables

1. **Declarative ML Pipelines**

   - Users write descriptors, not implementation code
   - Descriptors are serializable, shareable, versioned

2. **Production Performance**

   - Generated code is as fast as hand-written optimized code
   - Specialization eliminates runtime branching

3. **FormProcessor Control**

   - Progress tracking, resource management built in
   - FormProcessor orchestrates ML without knowing implementation details

4. **Reproducible Science**

   - Descriptor = complete specification
   - Share descriptor → reproduce results exactly

5. **Rapid Experimentation**
   - Change descriptor → regenerate → recompile
   - No manual code changes needed

---

## Next: Implementing Codegen Hooks

Phase 2.3 builds the ML architecture with **codegen hooks** - clear points where generation should plug in:

- `#[derive(ModelBuilder)]` - Generate model builders
- `#[derive(TrainingLoopGenerator)]` - Generate training loops
- `#[derive(FeaturePipelineGenerator)]` - Generate feature pipelines
- `#[derive(EvaluationGenerator)]` - Generate evaluation code

The **Meta Macro Processor** implementation comes after Phase 2.3, but Phase 2.3 ensures the ML architecture is **codegen-ready**.
